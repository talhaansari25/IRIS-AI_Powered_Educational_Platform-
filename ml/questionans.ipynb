{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU in use: NVIDIA GeForce MX350\n",
      "Processing your question...\n",
      "\n",
      "Answer:  MAN refers to Metropolitan Area Network. It is a local area network that connects computers and devices within a city or town.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import torch\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def check_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available. GPU in use:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "def get_similar_sentence(paragraph, query):\n",
    "    \"\"\"\n",
    "    Find the most similar sentence in the paragraph to the user's query.\n",
    "    \"\"\"\n",
    "    # Initialize the SentenceTransformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Split paragraph into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', paragraph.strip())\n",
    "\n",
    "    # Encode all sentences in the paragraph\n",
    "    paragraph_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Encode the user's query\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarity between the query and each sentence in the paragraph\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, paragraph_embeddings)[0]\n",
    "\n",
    "    # Get the sentence with the highest cosine similarity\n",
    "    best_match_index = torch.argmax(cosine_scores)\n",
    "    best_match_sentence = sentences[best_match_index]\n",
    "\n",
    "    return best_match_sentence\n",
    "\n",
    "def ask_question(paragraph, question):\n",
    "    # Find the most relevant sentence from the paragraph for the user's question\n",
    "    context = get_similar_sentence(paragraph, question)\n",
    "\n",
    "    # Define the prompt to ask the model\n",
    "    model = \"gemma:2b\"\n",
    "    prompt = f\"\"\"\n",
    "    Given the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Answer the following question:\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Provide a short and precise answer without any additional explanations.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Processing your question...\")\n",
    "    response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    answer = response['message']['content']\n",
    "    return answer.strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_cuda()\n",
    "\n",
    "    paragraph = \"\"\"\n",
    "Hello everyone, welcome back to the computer networks course. Today we will see the classification of computer networks. After completing this session, we will understand what are LAN, MAN and WAN. And after understanding LAN, WAN and MAN, we will also understand what are the devices that are involved in LAN, MAN and WAN. And we will also understand what are the new trends in computer network.\n",
    "    \"\"\"\n",
    "\n",
    "    # User input for the query\n",
    "    user_query = input(\"Please enter your question related to the paragraph: \")\n",
    "\n",
    "    # Get the answer from the model\n",
    "    answer = ask_question(paragraph, user_query)\n",
    "\n",
    "    # Display the result\n",
    "    print(\"\\nAnswer: \", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
